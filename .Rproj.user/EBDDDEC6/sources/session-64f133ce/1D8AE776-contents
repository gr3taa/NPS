---
title: "Bike"
author: "Dello Russo, Deviardi, Gorbani"
date: "2025-02-11"
output:
  
  html_document: 
    df_print: paged
    toc: true
  pdf_document: default
  html_notebook: 
    df_print: paged
  word_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(rgl)
knitr::opts_chunk$set(
  echo = TRUE,        # Mostra il codice
  results = "hide"   # Nasconde i grafici
)
knitr::knit_hooks$set(webgl = hook_webgl)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
ggplot2::theme_set(ggplot2::theme_bw())
```


```{css, echo=FALSE}
.extracode {
background-color: lightblue;
}
```

# Libraries

```{r}

```


# Dataset 

```{r}
day <- read.csv("Dataset/day.csv")
hour <- read.csv("Dataset/hour.csv")
library(dplyr)
library(tidyr)
#controllo quante osservazioni orarie per ogni giorno
blocchi <- rle(hour$dteday)
blocchi$values   
blocchi$lengths

#Trasforma `dati_orari` per avere una colonna per ogni ora
hour_wide <- hour %>%
  pivot_wider(
    names_from = hr, 
    values_from = cnt,
    names_prefix = "cnt_hour_"
  )
#dataset che compatta le righe
hour_wide_sommato <- hour_wide %>%
  group_by(dteday) %>%
  summarise(across(starts_with("cnt_hour"), sum, na.rm = TRUE))
#Unisci `dati_giornalieri` e `dati_orari_wide` per data
day <- day %>%
  left_join(hour_wide_sommato, by = "dteday")

prop_casual <- day$casual/day$cnt
library(knitr)
library(kableExtra)
kable(head(day[,1:16]), caption = "Prime righe del dataset day") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```


# Grafici iniziali

```{r}
matplot(t(day[,17:40]), type='l',ylab="users", xlab="hour", main="Rentals by hour")

mese_anno <- paste(day$yr, day$mnth, sep = "_")
rle <- rle(mese_anno)
len <- rle$lengths
color_vector <- rainbow(24)
col <- rep (color_vector, len)
matplot(t(day[,17:40]), type='l', col=col)

col_holyday <- ifelse(day$holiday ==1, 'red', 'blue')
matplot(t(day[,17:40]), type='l', col=col_holyday)

col_working <- ifelse(day$workingday==1, 'red', 'blue')
matplot(t(day[,17:40]), type='l', col=col_working)
matplot(t(day[, 17:40]), 
        type = 'l', 
        col = ifelse(day$workingday == 1, 'blue', 
                     ifelse(day$holiday == 1, 'red', 'green')),xlab="hour", ylab="users")
legend("topleft", legend=c("working day","weekend", "holiday"), col=c("blue","green","red"), lty=1, lwd=2, cex=0.75)
legend("topleft", legend = c("working day","weekend", "holiday"),fill=c("blue","green","red"), text.col = c("blue","green","red"),cex = 0.8)

col_weather <- rainbow(4)[day$weathersit]
matplot(t(day[,17:40]), type='l', col=col_weather)

plot(prop_casual, col=col_working, pch=16)
shapiro.test(prop_casual[col_working=='blue'])

ggplot(day, aes(x = as.factor(weathersit), y = cnt)) +
  geom_boxplot() +
  labs(title = "Rentals by Weather Situation", x = "Weather Situation", y = "Total Rentals")

meteo<-day[,10:16]
meteo<-cbind(meteo,day[,16])
library(corrplot)
cor_meteo<-cor(meteo)
corrplot(cor_meteo, method = "number")

col_anno<-ifelse(day$yr==0, 'green', 'red')
matplot(t(day[,17:40]), type='l', col=col_anno,xlab="hour", ylab="users")
legend("topleft", legend=c("2011","2012"), col=c("red","blue"), lty=1, lwd=2, cex=0.75)

```

# 'Standardizzazione' dati

```{r}
# anova cnt-year
anno<-as.factor(day$yr)
fit <- aov(day$cnt~anno)
g<-nlevels(anno)
n<-dim(day)[1]
summary(fit)
plot(anno, day$cnt, xlab='anno',col=rainbow(g),main='Original Data')
T0 <- summary(fit)[[1]][1,4]  # extract the test statistic
B<-1000
T_stat <- numeric(B) 
for(perm in 1:B){
  # Permutation:
  permutation <- sample(1:n)
  cnt_perm <- day$cnt[permutation]
  fit_perm <- aov(cnt_perm ~ anno)
  
  # Test statistic:
  T_stat[perm] <- summary(fit_perm)[[1]][1,4]
}
hist(T_stat,xlim=range(c(T_stat,T0)),breaks=30)
abline(v=T0,col=3,lwd=2)
p_val <- sum(T_stat>=T0)/B
p_val#0
# standardizzazione
cnt2011 <- day$cnt[1:365]
cnt2012 <- day$cnt[c(366:424,426:731)]
rapp1 <- 1/mean(cnt2011[1:79]/cnt2012[1:79])
rapp2 <- 1/mean(cnt2011[80:171]/cnt2012[80:171])
rapp3 <- 1/mean(cnt2011[172:265]/cnt2012[172:265])
rapp4 <- 1/mean(cnt2011[c(266:301,304:354)]/cnt2012[c(266:301,304:354)])
cnt_rapp <- c(day$cnt[1:365],cnt2012[1:79]/rapp1,cnt2012[80:171]/rapp2,cnt2012[172:265]/rapp3,cnt2012[266:354]/rapp4,cnt2012[355:365])
cnt_rapp <- c(cnt_rapp[1:424],day$cnt[425]/rapp1,cnt_rapp[425:730])

cnt_diff <- day$cnt - 2204*day$yr
se_diff <- sum((cnt_diff[1:365]-cnt_diff[c(366:424,426:731)])^2)
se_rapp <- sum((cnt_rapp[1:365]-cnt_rapp[c(366:424,426:731)])^2)
plot(cnt_rapp[1:365], col=rep('red',365))
points(cnt_rapp[c(366:424,426:731)], col=rep('blue',36))
# anova cnt_rapp-year
fit <- aov(cnt_rapp~anno)
summary(fit)
plot(anno, cnt_rapp, xlab='anno',col=rainbow(g),main='Original Data')
T0 <- summary(fit)[[1]][1,4]  # extract the test statistic
T_stat <- numeric(B) 
for(perm in 1:B){
  # Permutation:
  permutation <- sample(1:n)
  cnt_perm <- cnt_rapp[permutation]
  fit_perm <- aov(cnt_perm ~ anno)
  
  # Test statistic:
  T_stat[perm] <- summary(fit_perm)[[1]][1,4]
}
hist(T_stat,xlim=range(c(T_stat,T0)),breaks=30)
abline(v=T0,col=3,lwd=2)
p_val <- sum(T_stat>=T0)/B
p_val #0.33
```

# Prima research question
## Test permutations su day$cnt in base a working day

```{r}
a<-subset(day,day$workingday==1)
matplot(t(a[,17:40]), type='l')
b<-subset(day,day$workingday==0)
matplot(t(b[,17:40]), type='l')
x<- a$cnt
y<-b$cnt
perm_t_test=function(x,y,iter=1e3){
  T0=abs(mean(x)-mean(y))  # define the test statistic
  T_stat=numeric(iter) # a vector to store the values of each iteration
  x_pooled=c(x,y) # pooled sample
  n=length(x_pooled)
  n1=length(x)
  print(sprintf("Using %s iterations to estimate the permutational distribution. There are actually %s possible permutations", iter, factorial(n)))
  for(perm in 1:iter){ # loop for conditional MC
    # permutation:
    permutation <- sample(1:n)
    x_perm <- x_pooled[permutation]
    x1_perm <- x_perm[1:n1]
    x2_perm <- x_perm[(n1+1):n]
    # test statistic:
    T_stat[perm] <- abs(mean(x1_perm) - mean(x2_perm))
  }
  # p-value
  p_val <- sum(T_stat>=T0)/iter
  return(p_val)
}
p.value <- perm_t_test(x,y,iter=1e3) #0.093
```

## Interval wise test

```{r}
library(fdatest)
library(data.table)
library(fda)
library(roahd)
fd<-fData(seq(0,23),(day[,17:40]))

w1_fd=subset(fd, day$weathersit==1)
plot(w1_fd)
w2_fd=subset(fd, day$weathersit==2)
plot(w2_fd)
w3_fd=subset(fd, day$weathersit==3)
plot(w3_fd)

day_hour <- as.matrix(day[,17:40])
groups <- day$workingday
ITP.result <- ITPaovbspline(day_hour ~ groups, B=1000,nknots=20,order=3,method="responses") #ITPlmbspline per la regressione
summary(ITP.result)
plot(ITP.result,plot.adjpval = TRUE, xrange=c(0,23), )
```

## Test anova in base a weathersit 

```{r}

T0_workingday <- summary.aov(aov(cnt_rapp ~ workingday + weathersit))[[1]][1,4]
# residuals under H0:
# cnt = mu + beta*weathersit
aov.H0workingday <- aov(cnt_rapp ~ weathersit)
residuals.H0workingday <- aov.H0workingday$residuals

# Test for weathersit
T0_weathersit <- summary.aov(aov(cnt_rapp ~ workingday + weathersit))[[1]][2,4]
# residuals under H0:
# cnt = mu + alpha*workingday
aov.H0weathersit <- aov(cnt_rapp ~ workingday)
residuals.H0weathersit <- aov.H0weathersit$residuals
B <- 1000
T_weathersit <- T_workingday <- numeric(B)
for(perm in 1:B){
  permutation <- sample(n)
  
  # Test workingday
  cnt.perm.H0workingday <- aov.H0workingday$fitted + residuals.H0workingday[permutation]
  T_workingday[perm] <- summary.aov(aov(cnt.perm.H0workingday ~ workingday + weathersit))[[1]][1,4]
  
  # Test weathersit
  cnt.perm.H0weathersit <- aov.H0weathersit$fitted + residuals.H0weathersit[permutation]
  T_weathersit[perm] <- summary.aov(aov(cnt.perm.H0weathersit ~ workingday + weathersit))[[1]][2,4]
}
sum(T_workingday >= T0_workingday)/B#0.144
sum(T_weathersit >= T0_weathersit)/B#0

```

## Test regressione “hour” covariata 

```{r}
attach(hour)
result<-lm(cnt ~ hum +temp + windspeed + hr)
summary(result)
#Let’s start with a global test
T0_glob <- summary(result)$adj.r.squared
T_H0glob <- numeric(B)
for(perm in 1:B){
  permutation <- sample(n)
  
  Y.perm.glob <- cnt[permutation]
  T_H0glob[perm] <- summary(lm(Y.perm.glob ~ hum +temp + windspeed +hr))$adj.r.squared
}
sum(T_H0glob>=T0_glob)/B#0

T0_x1 <- abs(summary(result)$adj.r.squared)
T0_x2 <- abs(summary(result)$adj.r.squared)
T0_x3 <- abs(summary(result)$adj.r.squared)
T0_x4 <- abs(summary(result)$adj.r.squared)

regr.H01 <- lm(cnt~temp+windspeed+hr)
residuals.H01 <- regr.H01$residuals

regr.H02 <- lm(cnt~hum+windspeed+hr)
residuals.H02 <- regr.H02$residuals

regr.H03 <- lm(cnt~hum+temp+hr)
residuals.H03 <- regr.H03$residuals

regr.H04 <- lm(cnt~hum+temp+windspeed)
residuals.H04 <- regr.H04$residuals

T_H01 <- T_H02 <- T_H03 <- T_H04 <- numeric(B)

for(perm in 1:B){
  permutation <- sample(n)
  
  residuals.H01.perm <- residuals.H01[permutation]
  Y.perm.H01 <- regr.H01$fitted + residuals.H01.perm
  T_H01[perm] <- abs(summary(lm(Y.perm.H01 ~ hum+temp+windspeed+hr))$adj.r.squared)
  
  residuals.H02.perm <- residuals.H02[permutation]
  Y.perm.H02 <- regr.H02$fitted + residuals.H02.perm
  T_H02[perm] <- abs(summary(lm(Y.perm.H02 ~ hum+temp+windspeed+hr))$adj.r.squared)
  
  residuals.H03.perm <- residuals.H03[permutation]
  Y.perm.H03 <- regr.H03$fitted + residuals.H03.perm
  T_H03[perm] <- abs(summary(lm(Y.perm.H03 ~ hum+temp+windspeed+hr))$adj.r.squared)
  
  residuals.H04.perm <- residuals.H04[permutation]
  Y.perm.H04 <- regr.H04$fitted + residuals.H04.perm
  T_H04[perm] <- abs(summary(lm(Y.perm.H04 ~ hum+temp+windspeed+hr))$adj.r.squared)
  
}

sum(T_H01>=T0_x1)/B#0
sum(T_H02>=T0_x2)/B#0
sum(T_H03>=T0_x3)/B#0.5
sum(T_H04>=T0_x4)/B#0

detach(hour)
```

## Test regressione “day” covariata

```{r}
attach(day2)
n<- dim(day2)[1]
result<-lm(registered ~ hum +temp + windspeed+instant)
summary(result)
shapiro.test(result$residuals)$p
qqnorm(result$residuals)
qqline(result$residuals)
#Let’s start with a global test
T0_glob <- summary(result)$adj.r.squared
T_H0glob <- numeric(B)
for(perm in 1:B){
  permutation <- sample(n)
  
  Y.perm.glob <- registered[permutation]
  T_H0glob[perm] <- summary(lm(Y.perm.glob ~ hum +temp +instant+ windspeed))$adj.r.squared
}
sum(T_H0glob>=T0_glob)/B#0

T0_x1 <- abs(summary(result)$adj.r.squared)
T0_x2 <- abs(summary(result)$adj.r.squared)
T0_x3 <- abs(summary(result)$adj.r.squared)
T0_x4<- abs(summary(result)$adj.r.squared)
regr.H01 <- lm(registered~temp+windspeed+instant)
residuals.H01 <- regr.H01$residuals

regr.H02 <- lm(registered~hum+windspeed+instant)
residuals.H02 <- regr.H02$residuals

regr.H03 <- lm(registered~hum+temp+instant)
residuals.H03 <- regr.H03$residuals

regr.H04 <- lm(registered~hum+temp+windspeed)
residuals.H04 <- regr.H04$residuals

T_H01 <- T_H02 <- T_H03 <- T_H04<- numeric(B)
set.seed(202454)
for(perm in 1:B){
  permutation <- sample(n)
  
  residuals.H01.perm <- residuals.H01[permutation]
  Y.perm.H01 <- regr.H01$fitted + residuals.H01.perm
  T_H01[perm] <- abs(summary(lm(Y.perm.H01 ~ hum+temp+windspeed+instant))$adj.r.squared)
  
  residuals.H02.perm <- residuals.H02[permutation]
  Y.perm.H02 <- regr.H02$fitted + residuals.H02.perm
  T_H02[perm] <- abs(summary(lm(Y.perm.H02 ~ hum+temp+windspeed+instant))$adj.r.squared)
  
  residuals.H03.perm <- residuals.H03[permutation]
  Y.perm.H03 <- regr.H03$fitted + residuals.H03.perm
  T_H03[perm] <- abs(summary(lm(Y.perm.H03 ~ hum+temp+windspeed+instant))$adj.r.squared)
  
  residuals.H04.perm <- residuals.H04[permutation]
  Y.perm.H04 <- regr.H04$fitted + residuals.H04.perm
  T_H04[perm] <- abs(summary(lm(Y.perm.H04 ~ hum+temp+windspeed+instant))$adj.r.squared)
}

sum(T_H01>=T0_x1)/B#0.008
sum(T_H02>=T0_x2)/B#0
sum(T_H03>=T0_x3)/B#0.127
sum(T_H04>=T0_x4)/B#0
detach(day2)
```

## trend
```{r}
####trend ####
nbasis <- 10 # Numero di funzioni di base (più alto = più dettagli, meno liscio)
bspline_basis <- create.bspline.basis(rangeval = c(1,731), nbasis = nbasis)

# 3. Effettuiamo lo smoothing con la penalizzazione lambda
lambda <- 1  # Parametro di smoothing (da calibrare)
fdParobj <- fdPar(bspline_basis, lambda = lambda)
smoothed_fd <- smooth.basis(1:731, day2$registered, fdParobj)$fd
func <- eval.fd(1:731, smoothed_fd)

# 4. Visualizziamo il risultato
plot(1:731, day$registered, pch = 16, col = "gray", main = "Smoothing con Basi di Fourier",
     xlab = "Tempo", ylab = "Valore")
lines(1:731, func, col = "blue", lwd = 2)  # Curva smussata
legend("topright", legend = c("Dati", "Smoothing"),
       col = c("gray", "blue"), lty = c(NA, 1, 2), pch = c(16, NA, NA))
stl_result <- stl(ts(func[,1], frequency = 364), s.window = "periodic")  
plot(stl_result)

```



## Regressione seperata per le due variabili

```{r}
model_linear_spline1 <- lm(cnt_rapp ~ bs(temp,degree=3, df=5), data=day)#D: knots=c(0.7)
temp.grid=(seq(range(temp)[1],range(temp)[2],by=0.01))
preds=predict(model_linear_spline1,list(temp=temp.grid),se=T)
se.bands=cbind(preds$fit +2* preds$se.fit ,preds$fit -2* preds$se.fit)
with(day, plot(temp ,cnt_rapp ,xlim=range(temp.grid) ,cex =.5, col =" darkgrey ",xlab="temperature", ylab="users"))
lines(temp.grid,preds$fit ,lwd =2, col =" blue")
matlines(temp.grid ,se.bands ,lwd =1, col =" blue",lty =3)

model_linear_spline2 <- lm(cnt_rapp ~ bs(hum,degree=3, df=5), data=day)#D: knots=c(0.7)
hum.grid=(seq(range(hum)[1],range(hum)[2],by=0.01))
preds=predict(model_linear_spline2,list(hum=hum.grid),se=T)
se.bands=cbind(preds$fit +2* preds$se.fit ,preds$fit -2* preds$se.fit)
with(day, plot(hum ,cnt_rapp ,xlim=range(hum.grid) ,cex =.5, col =" darkgrey ",xlab="humidity", ylab="users"))
lines(hum.grid,preds$fit ,lwd =2, col =" blue")
matlines(hum.grid ,se.bands ,lwd =1, col =" blue",lty =3)

```

```{r}
temp.grid <- seq(range(day$temp)[1], range(day$temp)[2], by = 0.5)
hum.grid  <- seq(range(day$hum)[1], range(day$hum)[2], by = 0.5)
inst.grid <- mean(day$instant)  # Usiamo un valore medio per 'instant'

vis.gam(model_gam_reduced, 
        view = c("temp", "hum"), 
        plot.type = "persp", 
        color = "terrain", 
        theta = 30, 
        phi = 30, 
        ticktype = "detailed", 
        plot.points = TRUE)


# Griglia dei dati
grid <- expand.grid(instant = inst.grid, hum = hum.grid, temp = temp.grid)


# Previsione
pred_tp <- predict(model_gam_reduced, newdata = grid)

# Conversione in matrice per il grafico 3D
z_matrix <- matrix(pred_tp, nrow = length(hum.grid), ncol = length(temp.grid))
open3d()
persp3d(x = temp.grid, y = hum.grid, z = z_matrix, col = "lightblue",
        xlab = "Temperature", ylab = "Humidity", zlab = "Predicted Rentals",
        theta = 40, phi = 20, expand = 0.8)

plot_ly(
  x = temp.grid,
  y = hum.grid,
  z = z_matrix,
  type = "surface"
) %>%
  layout(
    title = "3D Surface Plot of Bike Rentals",
    scene = list(
      xaxis = list(title = "Temperature"),
      yaxis = list(title = "Humidity"),
      zaxis = list(title = "Predicted Rentals")
    )
  )


```



## modello thin plate spline

```{r}
library(mgcv)
attach(day)
grid=expand.grid(hum.grid,temp.grid)
names(grid)=c('hum','temp')
model_gam_tp = gam(cnt_rapp ~ s(hum, temp, bs="tp", m=2), # m for order
                   data = day)
pred_tp = predict(model_gam_tp,
                  newdata = (grid))
open3d()
persp3d(hum.grid,temp.grid, pred_tp, col = 'grey30')
points3d(hum, temp, cnt_rapp, col = 'red', size = 5)
detach(day)
```

## Modello gam e test confronto

```{r}
model_gam_inter=gam(cnt ~ instant + s(hum,bs='cr')+s(temp,bs='cr')+s(I(hum*temp),bs='cr'),data = day)
summary(model_gam_inter)

model_gam_reduced=gam(cnt ~ instant + s(hum,bs='cr')+s(temp,bs='cr'),data = day)
summary(model_gam_reduced)

model_gam_tp = gam(cnt ~ instant + s(hum, temp, bs="tp", m=2), # m for order
                   data = day)

temp.grid=(seq(range(day$temp)[1],range(day$temp)[2],by=0.01))
hum.grid=(seq(range(day$hum)[1],range(day$hum)[2],by=0.01))
ist.grid=(seq(range(day$instant)[1],range(day$instant)[2],by=1))
grid=expand.grid(instant = ist.grid, hum = hum.grid, temp = temp.grid)
grid=as.data.frame(grid)
pred_tp = predict(model_gam_reduced,
                  newdata = grid)
z_matrix <- matrix(pred_tp, nrow = length(hum.grid), ncol = length(temp.grid))

open3d()
persp3d(hum.grid,temp.grid, pred_tp, col = 'grey30')
points3d(day$hum, day$temp, day$cnt, col = 'red', size = 5)

hum.grid <- seq(min(day$hum), max(day$hum), length = 30)  # 30 punti tra min e max di hum
temp.grid <- seq(min(day$temp), max(day$temp), length = 30)  # 30 punti tra min e max di temp

# Creare una griglia combinando tutti i valori di hum e temp
grid <- expand.grid(hum = hum.grid, temp = temp.grid, instant = 1)
pred_tp <- predict(model_gam_reduced, newdata = grid)
pred_tp_matrix <- matrix(pred_tp, nrow = length(hum.grid), ncol = length(temp.grid))
open3d()
persp3d(hum.grid, temp.grid, pred_tp_matrix, col = 'grey30', alpha = 0.7, 
        xlab = "Humidity", ylab = "Temperature", zlab = "Bike Rentals")

# Aggiungere i punti reali in rosso
points3d(day$hum, day$temp, day$cnt, col = 'red', size = 5)
predict(model_gam_reduced, newdata = data.frame(hum = 0.51, temp = 11/41,instant=739))
predict(model_gam_inter, newdata = data.frame(hum = 0.51, temp = 11/41,instant=739))

anova(model_gam_tp ,model_gam_inter, test = "F")
shapiro.test(model_gam_tp$residuals)


summary.aov(aov(cnt ~ instant + s(hum, temp, bs="tp", m=2), # m for order
                data = day))
attach(day)
T0_station_fuel <-summary(cnt ~ instant + s(hum,bs='cr')+s(temp,bs='cr')+s(I(hum*temp),bs='cr') # m for order
                                  )
T0_station_fuel

# permutation
# the idea is to permute the residuals under H0:
# km = mu + alpha*station + beta*fuel
# additive model
aov.H0station_fuel <- aov(km ~ station + fuel)
aov.H0station_fuel
residuals.H0station_fuel <- aov.H0station_fuel$residuals
n <- 8
permutation <- sample(1:n)
residuals.H0station_fuel <- residuals.H0station_fuel[permutation]
# permuted y values:
km.perm.H0station_fuel <- aov.H0station_fuel$fitted + residuals.H0station_fuel
summary.aov(aov(km.perm.H0station_fuel ~ station + fuel + station:fuel))
# How data has changed?
layout(rbind(1:2))
plot(station_fuel, km, col=rainbow(5)[2:5], ylim=c(0,24),main='Original data')
plot(station_fuel, km.perm.H0station_fuel, col=rainbow(5)[2:5], ylim=c(0,24),main='Permuted data')



# Number of permutations
n_permutations <- 1000

# Placeholder to store permuted deviance differences
permuted_diffs <- numeric(n_permutations)

# Loop over permutations
F_value<-numeric(n_permutations)
for (i in 1:n_permutations) {
  # Permute the response variable (or predictors)
  permuted_data <- day
  permuted_data$y <- sample(day$cnt)  # Shuffle the outcome variable
  
  # Fit the reduced and full models to the permuted data
  model_gam_inter_perm=gam(y ~ instant + s(hum,bs='cr')+s(temp,bs='cr')+s(I(hum*temp),bs='cr'),data = permuted_data)

  model_gam_reduced_perm=gam(y ~ instant + s(hum,bs='cr')+s(temp,bs='cr'),data = permuted_data)
 
  # Calculate the deviance for both models
  RSS_full_perm <- deviance(model_gam_inter_perm)
  RSS_reduced_perm <- deviance(model_gam_reduced_perm)
  df_full <- N-sum(summary(model_gam_inter_perm)$s.table[,2])-1 # -1 for the intercept
  df_reduced <- N-sum(summary(model_gam_reduced_perm)$s.table[,2])-2 # -2 for the intercept and the linear contribution of education
  df_difference <- df_reduced-df_full
  F_value[i]<- ((RSS_full_perm-RSS_reduced_perm)/df_difference)/(RSS_full_perm/summary(model_gam_inter_perm)$residual.df)
  
  
  # Calculate the deviance difference for this permutation
  permuted_diffs[i] <- RSS_reduced_perm - RSS_full_perm
}

# Calculate the observed deviance difference
RSS_full <- deviance(model_gam_inter)
RSS_reduced <- deviance(model_gam_reduced)
observed_diff <- RSS_reduced - RSS_full
N<-731
df_full <- N-sum(summary(model_gam_inter)$s.table[,2])-1 # -1 for the intercept
df_reduced <- N-sum(summary(model_gam_reduced)$s.table[,2])-2 # -2 for the intercept and the linear contribution of education
df_difference <- df_reduced-df_full
F_value_manual <- ((RSS_reduced-RSS_full)/df_difference)/(RSS_full/summary(model_gam_inter)$residual.df)
# Compute p-value by comparing observed difference with permuted distribution
p_value_permutation <- mean(F_value >= F_value_manual)
print(p_value_permutation)#0.425

permuted_diffs <- numeric(n_permutations)

# Loop over permutations
F_value<-numeric(n_permutations)
for (i in 1:n_permutations) {
  # Permute the response variable (or predictors)
  permuted_data <- day
  permuted_data$y <- sample(day$cnt)  # Shuffle the outcome variable
  
  # Fit the reduced and full models to the permuted data
  model_gam_inter_perm=gam(y ~ instant + s(hum, temp, bs="tp", m=2), # m for order
                           data = permuted_data)
  model_gam_reduced_perm=gam(y ~ instant + s(hum,bs='cr')+s(temp,bs='cr'),data = permuted_data)
  
  # Calculate the deviance for both models
  RSS_full_perm <- deviance(model_gam_inter_perm)
  RSS_reduced_perm <- deviance(model_gam_reduced_perm)
  df_full <- N-sum(summary(model_gam_inter_perm)$s.table[,2])-1 # -1 for the intercept
  df_reduced <- N-sum(summary(model_gam_reduced_perm)$s.table[,2])-2 # -2 for the intercept and the linear contribution of education
  df_difference <- df_reduced-df_full
  F_value[i]<- ((RSS_full_perm-RSS_reduced_perm)/df_difference)/(RSS_full_perm/summary(model_gam_inter_perm)$residual.df)
  
  
  # Calculate the deviance difference for this permutation
  permuted_diffs[i] <- RSS_reduced_perm - RSS_full_perm
}

# Calculate the observed deviance difference
RSS_full <- deviance(model_gam_tp)
RSS_reduced <- deviance(model_gam_reduced)
observed_diff <- RSS_reduced - RSS_full
N<-731
df_full <- N-sum(summary(model_gam_tp)$s.table[,2])-1 # -1 for the intercept
df_reduced <- N-sum(summary(model_gam_reduced)$s.table[,2])-2 # -2 for the intercept and the linear contribution of education
df_difference <- df_reduced-df_full
F_value_manual <- ((RSS_reduced-RSS_full)/df_difference)/(RSS_full/summary(model_gam_tp)$residual.df)
# Compute p-value by comparing observed difference with permuted distribution
p_value_permutation <- mean(F_value >= F_value_manual)
print(p_value_permutation)#0.425

```


## Conformal prediction 

```{r}
library(dplyr) 
library(ggplot2)
library(knitr)
library(broom)
library(tidyr)
library(progress)
library(pbapply)
library(dbscan)
library(gridExtra)
library(fda)
library(roahd)
library(conformalInference.fd)

day2011= subset(day, day$yr==0)
day2012= subset(day, day$yr==1)

a<-subset(day,day$workingday==1)
a2011 = subset(day2011,day2011$workingday==1)
a2012 = subset(day2012,day2012$workingday==1)

b<-subset(day,day$workingday==0)
b2011 = subset(day2011,day2011$workingday==0 & day2011$holiday==0)
b2012 = subset(day2012,day2012$workingday==0 & day2012$holiday==0)

par(mfrow=c(1,2))
matplot(t(a2012[,17:40]), type='l', col='darkorange', ylim = c(0,1100), main = "2012 working day")
matplot(t(b2012[,17:40]), type='l', col = 'forestgreen', ylim = c(0,1100), main= "2012 weekend")

# 2012 workingday 5-20
dataset <- a2012
x.grid <- 5:20
f_data <- fData(x.grid, dataset[,22:37])
rho <- 0.5 # train and calibration split ratio

alpha = 0.1

l <- dim(dataset)[1] - ceiling(dim(dataset)[1] * rho)
alpha_rag <- c(seq(1/(l+1), l/(l+1), 1/(l+1)))

fun=mean_lists()
x0=list(as.list(x.grid))

band <- conformal.fun.split(NULL, NULL, f_data, NULL, x0, fun$train.fun, fun$predict.fun,
                            alpha = alpha,
                            split=NULL, seed=2024, randomized=FALSE, seed.rand=FALSE,
                            verbose=FALSE, rho=rho, s.type="st-dev")

upper_b_a <- band$up[[1]][[1]]
lower_b_a <- band$lo[[1]][[1]]
point_pred_a <- band$pred[[1]][[1]]

# 2012 weekend 5-20
dataset <- b2012
x.grid <- 5:20
f_data <- fData(x.grid, dataset[,22:37])
rho <- 0.5 # train and calibration split ratio

alpha = 0.1

l <- dim(dataset)[1] - ceiling(dim(dataset)[1] * rho)
alpha_rag <- c(seq(1/(l+1), l/(l+1), 1/(l+1)))

fun=mean_lists()
x0=list(as.list(x.grid))

band <- conformal.fun.split(NULL, NULL, f_data, NULL, x0, fun$train.fun, fun$predict.fun,
                            alpha = alpha,
                            split=NULL, seed=2024, randomized=FALSE, seed.rand=FALSE,
                            verbose=FALSE, rho=rho, s.type="st-dev")

upper_b_b <- band$up[[1]][[1]]
lower_b_b <- band$lo[[1]][[1]]
point_pred_b <- band$pred[[1]][[1]]


par(mfrow=c(1,2))
plot(x.grid, upper_b_a, type='l', col="black", lty=2, xlab="hour", ylab="cnt", lwd=1.5,
     ylim=c(min(lower_b_b),max(upper_b_a)),
     main="Functional CP band working day")
lines(x.grid, point_pred_a, type='l', col="darkorange", lwd=1.5)
lines(x.grid, lower_b_a, type='l', col="black", lty=2, lwd=1.5)

plot(x.grid, upper_b_b, type='l', col="black", lty=2, xlab="hour", ylab="cnt", lwd=1.5,
     ylim=c(min(lower_b_b),max(upper_b_a)),
     main="Functional CP band weekend")
lines(x.grid, point_pred_b, type='l', col="forestgreen", lwd=1.5)
lines(x.grid, lower_b_b, type='l', col="black", lty=2, lwd=1.5)

par(mfrow=c(1,1))
```

# seconda research question

## dataset 

```{r}
hour_wide_register <- hour %>%
  pivot_wider(
    names_from = hr, 
    values_from = registered,
    names_prefix = "cnt_hour_"
  )
hour_wide_casual <- hour %>%
  pivot_wider(
    names_from = hr, 
    values_from = casual,
    names_prefix = "cnt_hour_"
  )
#dataset che compatta le righe
hour_wide_sommato_reg <- hour_wide_register %>%
  group_by(dteday) %>%
  summarise(across(starts_with("cnt_hour_"), sum, na.rm = TRUE))
hour_wide_sommato_cas <- hour_wide_casual %>%
  group_by(dteday) %>%
  summarise(across(starts_with("cnt_hour_"), sum, na.rm = TRUE))
day1 <- day %>%
  left_join(hour_wide_sommato_cas, by = "dteday")
day2<- day %>%
  left_join(hour_wide_sommato_reg, by = "dteday")
matplot(t(hour_wide_sommato_reg[,2:25]), type='l')
matplot(t(hour_wide_sommato_cas[,2:25]), type='l')

```

## Test casual vs registered

```{r}
t1 = day1[,41:64]
t2 = day2[,41:64]

t1.mean = colMeans(t1)
t2.mean = colMeans(t2)

matplot(seq(0,23),
        t(rbind(t1.mean,t2.mean)), 
        type='l', col=c(1,2), lty=1,
        main="Sample multivariate means")

n1 = dim(t1)[1]
n2 = dim(t2)[1]
n  = n1 + n2

T20 = as.numeric(t(t1.mean-t2.mean) %*% (t1.mean-t2.mean))  # matrix (vector) product
#T20 <- norm(t1.mean-t2.mean) same as before
T20
B<-1000
T2 = numeric(B)
set.seed(19)
for(perm in 1:B){
  # Random permutation of indexes
  # When we apply permutations in a multivariate case, we keep the units together
  # i.e., we only permute the rows of the data matrix
  t_pooled = rbind(t1,t2)
  permutation = sample(n)
  t_perm = t_pooled[permutation,]
  t1_perm = t_perm[1:n1,]
  t2_perm = t_perm[(n1+1):n,]
  
  # Evaluation of the test statistic on permuted data
  t1.mean_perm = colMeans(t1_perm)
  t2.mean_perm = colMeans(t2_perm)
  T2[perm]  = t(t1.mean_perm-t2.mean_perm) %*% (t1.mean_perm-t2.mean_perm) 
}
hist(T2,xlim=range(c(T2,T20)))
abline(v=T20,col=3,lwd=4)
p_val = sum(T2>=T20)/B
p_val#0


matplot(seq(0,23),colMeans(t2), type='l', col='red', lty=1, xlab='hour', ylab='cnt')
lines(seq(0,23),colMeans(t1), col='blue', lty=1)
legend("topleft", legend=c("registered", "casual"), col=c("red", "blue"), lty=1)
```

## standardizzazione

```{r}
day_casual <- day$casual
cnt2011 <- day_casual[1:365]
cnt2012 <- day_casual[c(366:424,426:731)]
plot(cnt2011, col=rep('red',365),ylim = c(0,max(cnt2012)))
points(cnt2012, col=rep('blue',365))
rapp1 <- 1/mean(cnt2011[1:79]/cnt2012[1:79])
rapp2 <- 1/mean(cnt2011[80:171]/cnt2012[80:171])
rapp3 <- 1/mean(cnt2011[172:265]/cnt2012[172:265])
rapp4 <- 1/mean(cnt2011[c(266:301,304:354)]/cnt2012[c(266:301,304:354)])
cnt_rapp <- c(day_casual[1:365],cnt2012[1:79]/rapp1,cnt2012[80:171]/rapp2,cnt2012[172:265]/rapp3,cnt2012[266:354]/rapp4,cnt2012[355:365])
cnt_rapp <- c(cnt_rapp[1:424],day_casual[425]/rapp1,cnt_rapp[425:730])
plot(cnt_rapp[1:365], col=rep('red',365))
points(cnt_rapp[c(366:424,426:731)], col=rep('blue',36))
day$cnt_rapp <- cnt_rapp

## ANOVA cnt causal-year-----------------------
B<-1000
yr<-as.factor(day$yr)
fit <- aov(day$casual~yr)
g<-nlevels(yr)
n<-dim(day)[1]
summary(fit)
plot(yr, day$casual, xlab='yr',col=rainbow(g),main='Original Data')
T0 <- summary(fit)[[1]][1,4]  # extract the test statistic
T_stat <- numeric(B) 
for(perm in 1:B){
  # Permutation:
  permutation <- sample(1:n)
  cnt_perm <- casual[permutation]
  fit_perm <- aov(cnt_perm ~ yr)
  
  # Test statistic:
  T_stat[perm] <- summary(fit_perm)[[1]][1,4]
}
hist(T_stat,xlim=range(c(T_stat,T0)),breaks=30)
abline(v=T0,col=3,lwd=2)
p_val <- sum(T_stat>=T0)/B
p_val#0


## ANoVA cnt_rapp-year-----------------------
B<-1000
yr<-as.factor(day$yr)
fit <- aov(cnt_rapp~yr)
g<-nlevels(yr)
n<-dim(day)[1]
summary(fit)
plot(yr, cnt_rapp, xlab='yr',col=rainbow(g),main='Original Data')
T0 <- summary(fit)[[1]][1,4]  # extract the test statistic
T_stat <- numeric(B) 
for(perm in 1:B){
  # Permutation:
  permutation <- sample(1:n)
  cnt_perm <- cnt_rapp[permutation]
  fit_perm <- aov(cnt_perm ~ yr)
  
  # Test statistic:
  T_stat[perm] <- summary(fit_perm)[[1]][1,4]
}
hist(T_stat,xlim=range(c(T_stat,T0)),breaks=30)
abline(v=T0,col=3,lwd=2)
p_val <- sum(T_stat>=T0)/B
p_val#0

par(mfrow=c(1,2))
plot(yr, casual, xlab='yr',col=rainbow(g),main='Original Data')
plot(yr, cnt_rapp, xlab='yr',col=rainbow(g),main='Original Data')
par(mfrow=c(1,1))
```

## Test regressione 

```{r}
attach(day)
n<- dim(day)[1]
result<-lm(casual ~ hum +temp + windspeed+instant,data=day)
summary(result)
shapiro.test(result$residuals)$p
qqnorm(result$residuals)
qqline(result$residuals)
#Let’s start with a global test
B<-1000
T0_glob <- summary(result)$adj.r.squared
T_H0glob <- numeric(B)
for(perm in 1:B){
  permutation <- sample(n)
  
  Y.perm.glob <- casual[permutation]
  T_H0glob[perm] <- summary(lm(Y.perm.glob ~ hum +temp + windspeed+instant))$adj.r.squared
}
sum(T_H0glob>=T0_glob)/B#0

T0_x1 <- abs(summary(result)$adj.r.squared)
T0_x2 <- abs(summary(result)$adj.r.squared)
T0_x3 <- abs(summary(result)$adj.r.squared)
T0_x4 <- abs(summary(result)$adj.r.squared)

regr.H01 <- lm(casual~temp+windspeed+instant)
residuals.H01 <- regr.H01$residuals

regr.H02 <- lm(casual~hum+windspeed+instant)
residuals.H02 <- regr.H02$residuals

regr.H03 <- lm(casual~hum+temp+instant)
residuals.H03 <- regr.H03$residuals

regr.H04 <- lm(casual~hum+temp+weathersit)
residuals.H04 <- regr.H04$residuals

T_H01 <- T_H02 <- T_H03 <- numeric(B)

for(perm in 1:B){
  permutation <- sample(n)
  
  residuals.H01.perm <- residuals.H01[permutation]
  Y.perm.H01 <- regr.H01$fitted + residuals.H01.perm
  T_H01[perm] <- abs(summary(lm(Y.perm.H01 ~ hum+temp+windspeed+instant))$adj.r.squared)
  
  residuals.H02.perm <- residuals.H02[permutation]
  Y.perm.H02 <- regr.H02$fitted + residuals.H02.perm
  T_H02[perm] <- abs(summary(lm(Y.perm.H02 ~ hum+temp+windspeed+instant))$adj.r.squared)
  
  residuals.H03.perm <- residuals.H03[permutation]
  Y.perm.H03 <- regr.H03$fitted + residuals.H03.perm
  T_H03[perm] <- abs(summary(lm(Y.perm.H03 ~ hum+temp+windspeed+instant))$adj.r.squared)
  
  residuals.H04.perm <- residuals.H04[permutation]
  Y.perm.H04 <- regr.H04$fitted + residuals.H04.perm
  T_H04[perm] <- abs(summary(lm(Y.perm.H04 ~ hum+temp+windspeed+instant))$adj.r.squared)
}

sum(T_H01>=T0_x1)/B#0.146
sum(T_H02>=T0_x2)/B#0
sum(T_H03>=T0_x3)/B#0.357 
sum(T_H04>=T0_x4)/B#0.031
detach(day)
```

## Modello quantile 

```{r}
library(quantreg)
temp.grid <- seq(range(day$temp)[1], range(day$temp)[2], by = 0.5)
with(day, plot(temp ,casual ,xlim=range(temp.grid) ,cex =.5, col =" darkgrey ",xlab="temperature", ylab="users"))
X <- model.matrix(casual ~ instant + bs(temp, df=15),data=day)
for(tau in (1:3)/4){
  fit <- rq(casual ~ instant + bs(temp, df=15), tau=0.95, data=day)
  accel.fit <- X %*% fit$coef
  with(day,lines(temp,accel.fit))
}

fit2 <- summary(rq(casual~temp,tau=c(.05, .25, .5, .75, .95),data=day))

plot(fit2,mfrow = c(1,2))

temp<-day2$temp
plot(temp,day2$casual,cex=.25,type="n",xlab="Temperature", ylab="Rentals")
points(temp,day2$casual,cex=.5,col="blue")
abline(rq(day2$casual~temp,tau=.5),col="blue") 
abline(lm(day2$casual~temp+instant),lty=2,col="red") #the dreaded ols line
taus <- c(.05,.1,.25,.75,.90,.95)

for( i in 1:length(taus)){
  abline(rq(day2$casual~temp+instant,tau=taus[i]),col="gray")
}

```


```{r}
library(quantreg)
# Esegui la quantile regression per il 10° percentile (bassa domanda)
model_low <- rq(casual ~ temp , data = day2, tau = 0.10)

# Esegui la quantile regression per il 90° percentile (alta domanda)
model_high <- rq(casual ~ temp , data = day2, tau = 0.90)

# Riepilogo del modello per la bassa domanda (10° percentile)
summary(model_low)

# Riepilogo del modello per l'alta domanda (90° percentile)
summary(model_high)

# Previsione per il 10° percentile (bassa domanda)
pred_low <- predict(model_low, newdata = data.frame(temp = c(0.2,0.5)))

# Previsione per il 90° percentile (alta domanda)
pred_high <- predict(model_high, newdata = data.frame(temp = c(0.2, 0.55)))

# Predizioni su una griglia di valori di temperatura e umidità
temp_values <- seq(min(day2$temp), max(day2$temp), by=0.01)


# Previsioni per il 10° e 90° percentile
predictions_low <- predict(model_low, newdata = data.frame(temp=temp_values))
predictions_high <- predict(model_high, newdata =data.frame(temp=temp_values))

plot(day2$temp, day2$casual, col = "black",  xlab = "Temperatura", ylab = "Domanda")
lines(temp_values, predictions_low, col = "red", lwd = 2)
lines(temp_values, predictions_high, col = "green", lwd = 2)
legend("topleft", legend = c("Dati", "10° Percentile", "90° Percentile"),
       col = c("black", "red", "green"), lty = 1, lwd = 2)
# Creazione di un grafico
library(ggplot2)
ggplot(grid, aes(x = temp, y = hum)) +
  geom_tile(aes(fill = predictions_low), alpha = 0.5) +
  geom_tile(aes(fill = predictions_high), alpha = 0.5) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Previsione della Domanda - Quantile Regression",
       x = "Temperatura (°C)", y = "Umidità (%)",
       fill = "Domanda")

# Previsione della domanda per il 10° percentile (bassa domanda)
ggplot(grid, aes(x = temp, y = casual)) +
  geom_tile(aes(fill = predictions_low), alpha = 0.7) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Previsione della Domanda - 10° Percentile (Bassa Domanda)",
       x = "Temperatura (°C)", y = "Umidità (%)",
       fill = "Domanda")

# Previsione della domanda per il 90° percentile (alta domanda)
ggplot(grid, aes(x = temp, y = hum)) +
  geom_tile(aes(fill = predictions_high), alpha = 0.7) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Previsione della Domanda - 90° Percentile (Alta Domanda)",
       x = "Temperatura (°C)", y = "Umidità (%)",
       fill = "Domanda")

# Creazione del grafico a contorno per il 10° percentile (bassa domanda)
library(ggplot2)

ggplot(grid, aes(x = temp, y = hum)) +
  geom_contour(aes(z = predictions_low, color = ..level..), size = 1) +
  scale_color_gradient(low = "blue", high = "red") +   # Colori dalla bassa alla alta domanda
  labs(title = "Domanda in Periodi di Bassa Richiesta (10° Percentile)",
       x = "Temperatura (°C)", y = "Umidità (%)",
       color = "Domanda") +
  theme_minimal()

# Heatmap per il 10° percentile (bassa domanda)
ggplot(grid, aes(x = temp, y = hum)) +
  geom_tile(aes(fill = predictions_low)) +            # Mappa di colore per la bassa domanda
  scale_fill_gradient(low = "blue", high = "red") +   # Colori dal blu (bassa) al rosso (alta)
  labs(title = "Domanda in Periodi di Bassa Richiesta (10° Percentile)",
       x = "Temperatura (°C)", y = "Umidità (%)",
       fill = "Domanda") +
  theme_minimal()

# Heatmap per il 90° percentile (alta domanda)
ggplot(grid, aes(x = temp, y = hum)) +
  geom_tile(aes(fill = predictions_high)) +           # Mappa di colore per l'alta domanda
  scale_fill_gradient(low = "blue", high = "red") +   # Colori dal blu (bassa) al rosso (alta)
  labs(title = "Domanda in Periodi di Alta Richiesta (90° Percentile)",
       x = "Temperatura (°C)", y = "Umidità (%)",
       fill = "Domanda") +
  theme_minimal()

fit1 <- rq(casual~temp+hum,tau=0.9,data=day2)
fit0 <- rq(casual~temp,tau=0.9,data=day2)
fit<- rq(casual~1,tau=0.9,data=day2)
shapiro.test(fit$residuals)
anova(fit0,fit,test="rank", score="wilcoxon")
attach(day2)

rq.alltau <- rq(casual~temp, tau=-1, data=day2)
ranks.h0 <- ranks(rq.alltau)

X1 <- model.matrix(fit1)
X0 <- model.matrix(fit0)
# obtain test statistic and parameters of distribution under the null
rank.test.params <- rq.test.rank(x0=X0,x1=(X1[,-1])[,-2], y=casual, v=rq.alltau, score="wilcoxon")
#trace(rq.test.rank, edit=T) 

# test statistic
rank.test.params$Tn
#degrees of freedom of chi-square distirb under the null
rank.test.params$ndf 
# noncentrality parameter of chi-square distirb under the null
rank.test.params$ncp  
# evaluate it wiht Monte Carlo (no need, of course)
grid <- seq(0,5, length.out=50)
plot(grid, sapply(grid, FUN=dchisq, df=as.numeric(rank.test.params$ndf), ncp=0), type="l", main="Approximate Distribution under H0")

# in a MC way
B <- 1e3
h0.monte.carlo <- rchisq(B,
                         df=as.numeric(rank.test.params$ndf), 
                         ncp=0
                  )
# p value
sum(as.numeric(rank.test.params$Tn) <= h0.monte.carlo)/B

plot(temp,casual,xlab = "milliseconds", ylab = "acceleration",type="n")
points(temp,casual,cex = .75)
library(splines)
X <- model.matrix(casual ~ bs(temp, df=5))
for(tau in c(0.1,0.9)){
 fit <- rq(casual ~ bs(temp, df=5), tau=tau, data=day2)
 accel.fit <- X %*% fit$coef
 lines(temp,accel.fit)
}

```


#causual stagioni

```{r}
col_season<-ifelse(day$season==1,"red",ifelse(day$season==2,"blue",ifelse(day$season==3,"green","black")))
col_weekend<-ifelse(day$workingday==1,"red","blue")
plot(day$temp,day$casual, col=col_weekend, xlab="casual", ylab="season")
legend("topleft", legend=c("workingday","weekend"), col=c("red","blue"), pch=1)
legend("topleft", legend=c( "winter","spring", "summer", "fall"), col=c("red", "blue", "green", "yellow"), pch=1)

day_work<-subset(day,day$workingday==1)
day_we<-subset(day,day$workingday==0)

fit_l<-lm(casual~temp,data=day2)
plot(day2$temp,fit_l$residuals,col=col_weekend)
abline(h=0,col='black')
shapiro.test(fit$residuals)

fit_l_work<-lm(casual~temp,data=day_work)
plot(day_work$temp,fit_l_work$residuals)
abline(h=0,col='black')
shapiro.test(fit$residuals)

fit_l_we<-lm(casual~temp,data=day_we)
plot(day_we$temp,fit_l_we$residuals)
abline(h=0,col='black')
shapiro.test(fit$residuals)

par(mfrow=c(1,2))
plot(day_work$temp,fit_l_work$residuals)
plot(day_we$temp,fit_l_we$residuals)

col_season_we<-ifelse(day_we$season==1,"red",ifelse(day_we$season==2,"blue",ifelse(day_we$season==3,"green","black")))
col_weather_we<-ifelse(day_we$weathersit==1,"red",ifelse(day_we$weathersit==2,"blue",ifelse(day_we$weathersit==3,"green","black")))
col_anno_we<-ifelse(day_we$yr==0,"red","blue")
plot(day_we$temp,day_we$casual, col=col_anno_we, xlab="casual", ylab="season")
legend("topleft", legend=c( "winter","spring", "summer", "fall"), col=c("red", "blue", "green", "black"), pch=1)
subset(day_we,day_we$season==4)
```

